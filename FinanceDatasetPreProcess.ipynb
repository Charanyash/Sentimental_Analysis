{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2ca6ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk\n",
    "# !pip install textblob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facc0d74",
   "metadata": {},
   "source": [
    "## 1) Loading The Text Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97e351ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "# nltk.download(\"punkt\")\n",
    "# nltk.download(\"stopwords\")\n",
    "# nltk.download('wordnet')\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "import numpy as np\n",
    "import time\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c89c276",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_variants = [\n",
    "    \"Sentences_50Agree\",\n",
    "    \"Sentences_66Agree\",\n",
    "    \"Sentences_75Agree\",\n",
    "    \"Sentences_AllAgree\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8beaa781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences_75Agree.txt\n",
      "Sentences_66Agree.txt\n",
      "Sentences_AllAgree.txt\n",
      "README.txt\n",
      "Sentences_50Agree.txt\n",
      "License.txt\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"/home/charanubuntu/NLP/FinancialPhraseBank-v1.0/FinancialPhraseBank-v1.0/\"\n",
    "files = os.listdir(folder_path)\n",
    "\n",
    "for i in files:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "325df354",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_variant = data_variants[2] + \".txt\"\n",
    "\n",
    "sentences = []\n",
    "\n",
    "label_map = {\"positive\" : 1 ,\"neutral\" : 0 , \"negative\" : -1}\n",
    "\n",
    "labels = []\n",
    "\n",
    "if chosen_variant in files :\n",
    "    \n",
    "    file_path = os.path.join(folder_path,chosen_variant)\n",
    "    \n",
    "    with open(file_path,encoding=\"iso-8859-1\") as file :\n",
    "        \n",
    "        for id_,line in enumerate(file):\n",
    "            x = re.split(\"@\",line)\n",
    "            \n",
    "            curr_label = re.split(r\"\\n\",x[1])\n",
    "            \n",
    "            sentences.append(x[0])\n",
    "            \n",
    "            labels.append(label_map[curr_label[0]])\n",
    "            \n",
    "                \n",
    "else :\n",
    "    print(\"No file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "834e94c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting the sentences and corresponding labels \n",
    "\n",
    "sent_arr = np.array(sentences)\n",
    "\n",
    "label_arr = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4c8856",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sent = sent_arr[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233ab4f8",
   "metadata": {},
   "source": [
    "## 2) Text Pre Processing Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5591d288",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remove Punctuations\n",
    "\n",
    "def remove_punctuation(sentences):\n",
    "    \n",
    "    for i in range(len(sentences)):\n",
    "        \n",
    "       sentences[i] =  re.sub(\"\\W\",\" \",sentences[i])\n",
    "\n",
    "    return sentences\n",
    "\n",
    "# Remove the stopwords\n",
    "\n",
    "def remove_stopwords(sentences):\n",
    "    \n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    \n",
    "    for i in range(len(sentences)) :\n",
    "        \n",
    "        filtered_words = [' ' + word for word in sentences[i].split() if word.lower() not in stop_words]\n",
    "        \n",
    "        sentences[i] = ''.join(filtered_words)\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "\n",
    "# Correcting the sentences \n",
    "\n",
    "def spell_check(sentences):\n",
    "    \n",
    "     for i in range(len(sentences)):\n",
    "        \n",
    "       sentences[i] = str(TextBlob(sentences[i]).correct())\n",
    "\n",
    "     return sentences\n",
    "    \n",
    "        \n",
    "# Tokenization \n",
    "\n",
    "def tokeniser(sentence):\n",
    "    \n",
    "    words = re.split(\"\\s+\",sentence)\n",
    "    \n",
    "    return words\n",
    "\n",
    "\n",
    "# Convert Capital to Small Letters\n",
    "\n",
    "\n",
    "def lemmatize_sent(sentence):\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    words = tokeniser(sentence)\n",
    "    \n",
    "    lem_tokens = []\n",
    "    \n",
    "    for word in words :\n",
    "        \n",
    "        word_pos = pos_tag([word])[0][1]\n",
    "        \n",
    "        pos_tag_map = {\n",
    "            'N': 'n',  # Noun\n",
    "            'V': 'v',  # Verb\n",
    "            'J': 'a',  # Adjective\n",
    "            'R': 'r'   # Adverb\n",
    "        }\n",
    "        \n",
    "        \n",
    "        pos = word_pos[0].upper()\n",
    "        \n",
    "        if pos in pos_tag_map:\n",
    "            lem = lemmatizer.lemmatize(word,pos = pos_tag_map[pos])\n",
    "        else :\n",
    "            lem = lemmatizer.lemmatize(word,pos = 'n')\n",
    "        \n",
    "        lem_tokens.append(lem.lower())\n",
    "    \n",
    "    return lem_tokens\n",
    "     \n",
    "\n",
    "def lower_case(tokens):\n",
    "    \n",
    "    for i in range(len(tokens)) :\n",
    "         tokens[i] = tokens[i].lower()\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "\n",
    "# Lemmatizing the tokens -> eg : running ,runs -> run, it is a process of grouping together different inflected forms of the same word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a814bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_without_punct = remove_punctuation(sent_arr) # Removed Punctuations \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e39c31a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spell_check_sentences = spell_check(sent_without_punct) # Correcting the spell mistakes\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "caa7689a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processed the sentences without any stop words\n",
    "\n",
    "# sentences_without_stp_wrds = remove_stopwords(spell_check_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3e8345c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "\n",
    "lem_tokens_sent = []\n",
    "\n",
    "for line in sent_without_punct :\n",
    "        \n",
    "   lem_tokens_sent.append(lemmatize_sent(line))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad9de864",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tok_sent = [sent[:-1] for sent in lem_tokens_sent]\n",
    "\n",
    "# Lower Casing the tokens \n",
    "tok_sent = [[word.lower() for word in sent] for sent in new_tok_sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "875bd016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['according',\n",
       " 'to',\n",
       " 'gran',\n",
       " 'the',\n",
       " 'company',\n",
       " 'have',\n",
       " 'no',\n",
       " 'plan',\n",
       " 'to',\n",
       " 'move',\n",
       " 'all',\n",
       " 'production',\n",
       " 'to',\n",
       " 'russia',\n",
       " 'although',\n",
       " 'that',\n",
       " 'be',\n",
       " 'where',\n",
       " 'the',\n",
       " 'company',\n",
       " 'be',\n",
       " 'grow']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_sent[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a978215f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "\n",
    "tokens = []\n",
    "\n",
    "for sent in tok_sent :\n",
    "    \n",
    "    for word in sent :\n",
    "        \n",
    "        tokens.append(word)\n",
    "     \n",
    "tokens = np.array(list(set(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "640e8a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7162\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8c28bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95f893c5",
   "metadata": {},
   "source": [
    "## 3) Saving the contents to a File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f116584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will save lem_tokens_sent and labels \n",
    "\n",
    "preprocessed_data = {\n",
    "    \"lem_tokens_sent\": tok_sent,\n",
    "    \"Vocabulary\" : tokens,\n",
    "    \"labels\": labels,   \n",
    "}\n",
    "\n",
    "\n",
    "with open(\"preprocessed_data.pkl\", \"wb\") as file:\n",
    "    pickle.dump(preprocessed_data, file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
